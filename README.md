Titanic Survival Prediction ğŸ§ ğŸš¢

**Repository:** Titanic Kaggle Dataset â€” Predicting survival on the Titanic using Machine Learning (Logistic Regression baseline + feature engineering)

---

## ğŸ Overview
This project uses the famous Titanic â€” Machine Learning from Disaster dataset from Kaggle to build a predictive model of passenger survival.  
I cleaned the data, engineered meaningful features, trained a Logistic Regression classifier (80% accuracy) and interpreted the results with feature importance.

---

## ğŸ” Key Insights
- **Sex (female)** increased survival odds by ~13Ã—.  
- **Pclass** had a strong negative effect (3rd class = worst odds).  
- **Larger families** aboard had lower odds (FamilySize engineered feature).  
- **Higher fare** correlated with higher survival.  
- **Port of embarkation** influenced survival (differences between Cherbourg, Southampton, Queenstown).

---

## ğŸ§° Tech Stack
- Python 3, Jupyter Notebook  
- pandas, numpy, matplotlib  
- scikit-learn (LogisticRegression)  
- GitHub for version control & sharing

---

## ğŸ“Š Files & Visuals
- `Titanic_Survival_Model.ipynb` â€“ full notebook & code  
- `/images/` â€“ feature importance chart, confusion matrix visual  
- README â€“ this file

---

## ğŸ How to Run
1. Clone the repo: `git clone https://github.com/VedantShettyy/Titanic_Kaggle_Dataset.git`  
2. Navigate to the notebook and open in Jupyter or Google Colab  
3. Run cells in order: data loading â†’ preprocessing â†’ feature engineering â†’ model training â†’ analysis  
4. Review the exported visuals and interpretation section

---

## ğŸ’¡ What Next?
I invite you to fork this project and try enhancements such as:
- Decision Tree / Random Forest / XGBoost  
- Hyperparameter tuning & cross-validation  
- Building a Streamlit app to allow interactive predictions  
- Alternative feature engineering (e.g., Title extraction from Name, Age buckets)

make this without any punctuations and hyphens only text
